{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity in Paragraph2Vec Text Representation\n",
    "\n",
    "*Gensim software examples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smart_open\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = ''\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection += doc.read()+'\\n'\n",
    "\n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "with open('gensim_data/all_gutenberg', 'w') as f:\n",
    "    f.write(doc_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(read_corpus('gensim_data/all_gutenberg'))\n",
    "paraph2vec = Doc2Vec(corpus, size=300, window=8, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling sentences from Str to p2v vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.split()\n",
    "sent2 = sentence2.split()\n",
    "\n",
    "vec_sent1_p2v = paraph2vec.infer_vector(sent1)\n",
    "vec_sent2_p2v = paraph2vec.infer_vector(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01495365, -0.52712882, -0.15033531, -0.03319004,  0.15571935,\n",
       "       -0.14664985,  0.04288134,  0.40894195,  0.1561143 , -0.37317038], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the vector of the word 'hall'.\n",
    "paraph2vec.wv['hall'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01261198, -0.02667871, -0.03645166, -0.00957741,  0.00195654,\n",
       "       -0.00025346, -0.00115494,  0.03957676, -0.00166694, -0.01274711], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the Paragraph2vector of the sentence 1\n",
    "print(len(vec_sent1_p2v))\n",
    "vec_sent1_p2v[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Paragraph2Vec sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-abe763561321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkullback_leibler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhellinger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcossim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_sent1_p2v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec_sent2_p2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for the next line to work the model must contain all appearing words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#paraph2vec.n_similarity(sentence1,sentence2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/flask/lib/python3.5/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mcossim\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmore\u001b[0m \u001b[0msimilar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \"\"\"\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvec1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "cossim(vec_sent1_p2v,vec_sent2_p2v)\n",
    "\n",
    "#for the next line to work the model must contain all appearing words\n",
    "#paraph2vec.n_similarity(sentence1,sentence2)\n",
    "\n",
    "#paraph2vec.wv.n_similarity(sentence1,sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Paragraph2Vec-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent1, sent2, model):\n",
    "\n",
    "    sentence1 = sent1.split()\n",
    "    sentence2 = sent2.split()\n",
    "    \n",
    "    p2v_sent1 = []\n",
    "    p2v_sent2 = []\n",
    "\n",
    "    for i,word in enumerate(sentence1):\n",
    "        try:\n",
    "            p2v_sent1.append(paraph2vec.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i,word in enumerate(sentence2):\n",
    "        try:\n",
    "            p2v_sent2.append(paraph2vec.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    p2v_sent1 = sum(np.asarray(p2v_sent1))\n",
    "    p2v_sent2 = sum(np.asarray(p2v_sent2))\n",
    "    \n",
    "    A = p2v_sent1.reshape(1,-1)\n",
    "    B = p2v_sent2.reshape(1,-1)\n",
    "    \n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.33344087, -3.05628347, -0.12873888,  0.28321588,  0.24815422,\n",
       "       -0.89516532, -0.85681403,  0.69070792,  0.43571717, -0.16906594], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v_sent1, p2v_sent2 = preproc_data(sentence1,sentence2,paraph2vec)\n",
    "print(len(p2v_sent1[0]))\n",
    "p2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78142464"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(p2v_sent1,p2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87112945"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "p2v_sent1s, p2v_sent2s = preproc_data(sent1s,sent2s,paraph2vec)\n",
    "cosine_similarity(p2v_sent1s,p2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.218575402618\n",
      "0.128870538611\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(p2v_sent1,p2v_sent2))\n",
    "print(cosine_scipy(p2v_sent1s,p2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69291740833683868"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraph2vec.n_similarity(['the','girl','run','into','the','hall'],['here','alice','run','to','the','hall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84913617439670053"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraph2vec.n_similarity(['girl','run','hall'],['alice','run','hall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60044111540961354"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraph2vec.n_similarity(['the','boy','eat','red','apple'],\n",
    "                   ['here','alice','run','to','the','hall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v infer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33136809"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing initial infer_vector similarity\n",
    "vec_sent1_p2v = vec_sent1_p2v.reshape(1,-1)\n",
    "vec_sent2_p2v = vec_sent2_p2v.reshape(1,-1)\n",
    "cosine_similarity(vec_sent1_p2v,vec_sent2_p2v)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33047721"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#infer_vector similarity filtering stopwords\n",
    "vec_sent1s_p2v = paraph2vec.infer_vector(sent1s.split()).reshape(1,-1)\n",
    "vec_sent2s_p2v = paraph2vec.infer_vector(sent2s.split()).reshape(1,-1)\n",
    "cosine_similarity(vec_sent1s_p2v,vec_sent2s_p2v)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v.similarity\n",
    "\n",
    "**Warning:** all the words must be converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69291735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "vec_sent1 = paraph2vec.wv[sent1]\n",
    "vec_sent2 = paraph2vec.wv[sent2]\n",
    "\n",
    "#cosine(vec_sent1,vec_sent2)\n",
    "vec_sent1_ = sum(vec_sent1).reshape(1,-1)\n",
    "vec_sent2_ = sum(vec_sent2).reshape(1,-1)\n",
    "\n",
    "cosine_similarity(vec_sent1_,vec_sent2_)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_cosine_sim(sent1, sent2,p2v):\n",
    "    for i,word in enumerate(sent1):\n",
    "        if i == 0:\n",
    "            sent1_p2v = p2v.wv[word]\n",
    "        else:\n",
    "            sent1_p2v+= p2v.wv[word]\n",
    "\n",
    "    for i,word in enumerate(sent2):\n",
    "        if i == 0:\n",
    "            sent2_p2v = p2v.wv[word]\n",
    "        else:\n",
    "            sent2_p2v+= p2v.wv[word]\n",
    "\n",
    "    # get the sentence vector similarity\n",
    "    return 1-cosine_scipy(sent1_p2v,sent2_p2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947779667627\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_cosine_sim(sent1,sent2,paraph2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like if paragraph2vec had a sparcity problem, due to that word vectors are to slow.\n",
    "Also the model if you test the *wv* method to many times the numers approximate to 0.999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap Similarity\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72026728389009687"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraph2vec.similarity('girl','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonic mean best pair-word similarity, stopword_filtering=no 0.670109130314\n",
      "Harmonic mean best pair-word similarity, stopword_filtering=yes 0.575509338622\n"
     ]
    }
   ],
   "source": [
    "sentence1 = ['the','girl','run','into','the','hall']\n",
    "sentence2 = ['Here','Alice','run','to','the','hall']\n",
    "\n",
    "def harmonic_best_pair_word_sim(string1,string2):\n",
    "    p=0\n",
    "    for wi in string1:\n",
    "        m = 0\n",
    "        for wc in string2:\n",
    "            try:\n",
    "                m = max(m, paraph2vec.similarity(wi,wc))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(string1)\n",
    "\n",
    "    q=0\n",
    "    for wc in string2:\n",
    "        m = 0\n",
    "        for wi in string1:\n",
    "            try:\n",
    "                m = max(m, paraph2vec.similarity(wi,wc))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(string2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim\n",
    "\n",
    "print('Harmonic mean best pair-word similarity, stopword_filtering=no',\n",
    "      harmonic_best_pair_word_sim(sentence1,sentence2))\n",
    "print('Harmonic mean best pair-word similarity, stopword_filtering=yes',\n",
    "      harmonic_best_pair_word_sim(['girl','run','hall'],['Alice','eat','hall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textsim Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textsim Jaccard 0.625\n",
      "Textsim Jaccard, stopwords_filter=yes 0.8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/abelm/')\n",
    "import textsim\n",
    "from textsim.tokendists import jaccard_distance\n",
    "print('Textsim Jaccard', jaccard_distance(sent1,sent2))\n",
    "print('Textsim Jaccard, stopwords_filter=yes', jaccard_distance('girl run hall','Alice eat hall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Same as Word2Vec this model doesn't works with bow structure, it represent a word as a vector of *size parameter* value length. At the same time this model can infered a vector for a sentence. The experiments shows that with the same corpus and the same sentences the paragraph2vec tends to fail with some words, e.g. 'Alice' or 'Here', this behavior is different to Word2Vec model.\n",
    "\n",
    "* Gensim Hellinger, Cosine, Jaccard, Kullback-Leibler and the others based on bowvec doesn't work.\n",
    "* 0.777 input = str, Jaccard, Textsim, stopwords_filter=no\n",
    "* 0.800 input = str, Jaccard, Textsim, stopwords_filter=yes\n",
    "* 0.433 input = str, Cosine, Textsim-sklearn, stopwords_filter=no\n",
    "* 0.333 input = str, Cosine, Textsim-sklearn, stopwords_filter=yes\n",
    "* 0.773 input = self vec, Cosine, Sklearn\n",
    "* 0.361 input = Doc2Vec infer_vec, Cosine, Sklearn\n",
    "* 0.635 input = str list, Harmonic mean, Best word sim of words in both sentences, stopwords_filter=no\n",
    "* 0.543 input = str list, Harmonic mean, Best word sim of words in both sentences, stopwords_filter=yes\n",
    "\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
