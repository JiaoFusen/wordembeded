{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing TfIdf Model with MSRPC\n",
    "\n",
    "**Prerequisites:** See the previous notebooks, and generate TfIdf model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice the application of Word Embedding models to a real corpus, the MSRPC, made for Paraphrase Recognition task.\n",
    "\n",
    "**Index:**\n",
    "- Data Wrangling:\n",
    "    - Loading TfIdf Model generated from Wikipedia dump.\n",
    "    - Load MSRPC\n",
    "    - Preprocessing (stopword removal, punctuation removal, etc.)\n",
    "    - From string to tfidf numerical vectors.\n",
    "- Feature Extraction:\n",
    "    - Calculate all similarity corpus-distances using tfidf.model for every pair of sentences in MSRP.\n",
    "- Classification:\n",
    "    - Training a simple machine model to recognize paraphrase using the distance vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas import DataFrame, Series, read_table, read_csv\n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Loading Wikipedia tfidf model, loading MSRP Corpus. Transforming MSRP string sentences to tfidf numerical vector sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    }
   ],
   "source": [
    "#Loading tfidf Wiki model and dictionary\n",
    "corpus_path = '/media/DATA/wiki_es/'\n",
    "tfidf = TfidfModel.load(corpus_path+'wiki-tfidf.model')\n",
    "dictionary = Dictionary.load_from_text(corpus_path+'_wordids.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data corpus length: 5801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ID1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>702876</td>\n",
       "      <td>702977</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2108705</td>\n",
       "      <td>2108831</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1330381</td>\n",
       "      <td>1330521</td>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3344667</td>\n",
       "      <td>3344648</td>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1236820</td>\n",
       "      <td>1236712</td>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class      ID1      ID2                                          sentence1  \\\n",
       "0      1   702876   702977  Amrozi accused his brother, whom he called \"th...   \n",
       "1      0  2108705  2108831  Yucaipa owned Dominick's before selling the ch...   \n",
       "2      1  1330381  1330521  They had published an advertisement on the Int...   \n",
       "3      0  3344667  3344648  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4      1  1236820  1236712  The stock rose $2.11, or about 11 percent, to ...   \n",
       "\n",
       "                                           sentence2  \n",
       "0  Referring to him as only \"the witness\", Amrozi...  \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...  \n",
       "2  On June 10, the ship's owners had published an...  \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...  \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Paraphrase Recognition corpus of Microsoft\n",
    "data = read_csv('data/msrpc.csv',sep='\\t',header=0)\n",
    "print('Data corpus length:', len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From str to bow vector\n",
    "def str_to_bow_tfidfVector(sentence,dictionary=dictionary,tfidf=tfidf):\n",
    "    vec_sent = dictionary.doc2bow(sentence.lower().split()) #sent bow vector\n",
    "    bow_sent_tfidf = tfidf[vec_sent]                        #sent tfidf vector\n",
    "    return bow_sent_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n",
      "[(8690, 0.3793547108014584), (19848, 0.49603841465983006), (23059, 0.2501920888751192), (23212, 0.46686708212150263), (41338, 0.5739990774122925)]\n"
     ]
    }
   ],
   "source": [
    "print(data.sentence1[0])\n",
    "print(str_to_bow_tfidfVector(data.sentence1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bow_tfidf vectors are generated because Gensim package have some similarity functions implemented that works with this kind of structure.\n",
    "\n",
    "But, we need a numpy array to work with sklearn or scipy text similarity functions. As a second condition to work with this sim func you need to normalize the # of words in both sentences, for the words that only appear in one sentence you need to assign a 0.0 value in the other, this make that the 2 sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From bow vector to numpy vector\n",
    "def bowtfidf_to_numpy(bow_tfidfVector1, bow_tfidfVector2):\n",
    "    vec2 = dict(bow_tfidfVector1)\n",
    "    vec1 = dict(bow_tfidfVector2)\n",
    "    nvec1,nvec2 = [],[]\n",
    "    words = set(vec1.keys()).union(vec2.keys())\n",
    "    for word in words:\n",
    "        nvec1.append(vec1.get(word,0.0))\n",
    "        nvec2.append(vec2.get(word,0.0))\n",
    "    \n",
    "    A = np.asarray(nvec1).reshape((1,-1))\n",
    "    B = np.asarray(nvec2).reshape((1,-1))\n",
    "    \n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.37133383 0.21991717 0.30226133 0.4985811\n",
      "  0.57694139 0.         0.34537114 0.1471734 ]] \n",
      " [[0.37935471 0.25019209 0.         0.         0.         0.49603841\n",
      "  0.57399908 0.46686708 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "bow_tfidfVector1 = str_to_bow_tfidfVector(data.sentence1[0])\n",
    "bow_tfidfVector2 = str_to_bow_tfidfVector(data.sentence2[0])\n",
    "np_tfidfVec1, np_tfidfVec2 = bowtfidf_to_numpy(bow_tfidfVector1,bow_tfidfVector2)\n",
    "print(np_tfidfVec1,'\\n' ,np_tfidfVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import hellinger\n",
    "from gensim.matutils import cossim as gcosine\n",
    "from gensim.matutils import jaccard as gjaccard\n",
    "#from scipy.spatial.distance import cosine as scosine\n",
    "from textsim.tokendists import cosine_distance_scipy as scosine\n",
    "from textsim.tokendists import cosine_similarity_sklearn as kcosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence. \n",
      " Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence. \n",
      "\n",
      "Hellinger distance:  1.1141090076668747\n",
      "Gensim Cosine distance: 0.5784792046014192\n",
      "Gensim Jaccard distance: 0.7687920514085964\n",
      "Scipy cosine distance:  0.5784792046014192\n",
      "Sklearn cosine distance:  0.5784792046014192\n"
     ]
    }
   ],
   "source": [
    "## Gensim TfIdf-Hellinger sentence similarity\n",
    "print(data.sentence1[0],'\\n',data.sentence2[0],'\\n')\n",
    "print('Hellinger distance: ',hellinger(bow_tfidfVector1,bow_tfidfVector2))\n",
    "print('Gensim Cosine distance:',gcosine(bow_tfidfVector1,bow_tfidfVector2))\n",
    "print('Gensim Jaccard distance:',gjaccard(bow_tfidfVector1,bow_tfidfVector2))\n",
    "print('Scipy cosine distance: ', scosine(np_tfidfVec1,np_tfidfVec2))\n",
    "print('Sklearn cosine distance: ', kcosine(np_tfidfVec1,np_tfidfVec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all features for all pair of sentences\n",
    "\n",
    "And adding those to a new pandas DataFrame for Machine Learning task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'hellinger':hellinger,\n",
    "    'gcosine':gcosine,\n",
    "    'gjaccard':gjaccard,\n",
    "    'scosine':scosine,\n",
    "    'kcosine':kcosine,\n",
    "}\n",
    "columns=list(features.keys())\n",
    "columns.append('class')\n",
    "\n",
    "df = DataFrame(columns=columns)\n",
    "for i in range(len(data)):\n",
    "    row = []\n",
    "    sent1 = str_to_bow_tfidfVector(data.sentence1[i],dictionary=dictionary,tfidf=tfidf)\n",
    "    sent2 = str_to_bow_tfidfVector(data.sentence2[i],dictionary=dictionary,tfidf=tfidf)\n",
    "    for feature in features:\n",
    "        if feature in ['scosine','kcosine']:\n",
    "            A,B = bowtfidf_to_numpy(sent1,sent2)\n",
    "            row.append(features[feature](A,B))\n",
    "        else:\n",
    "            row.append(features[feature](sent1,sent2))\n",
    "    \n",
    "    row.append(data.iloc[i]['class'])\n",
    "    df.loc[i] = row\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scosine      0.578479\n",
       "hellinger    1.114109\n",
       "kcosine      0.578479\n",
       "gcosine      0.578479\n",
       "gjaccard     0.768792\n",
       "class        1.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase Recognition Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.as_matrix(list(features.keys()))\n",
    "y = df.as_matrix(['class']).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "Xs = scale(x,with_mean=True,with_std=True,axis=0)\n",
    "\n",
    "#Partitioning data into test and train subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# fit the model with data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6576680068925904"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# STEP 3: make predictions on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# compare actual response values (y_test) with predicted response values (y_pred)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know how tfidf impact the result, look at the accuracy not using a tfidf model. Now as you don't have the TfIdf model Gensim cosine, hellinger or jaccard can't be used. At the same time we used a vector to call sklearn and sciy measures, but now without a word vector model we need the original string sentences. We will use the same measures loaded from textsim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textsim.tokendists import cosine_distance_scipy as tscosine\n",
    "from textsim.tokendists import cosine_distance_sklearn as tkcosine\n",
    "\n",
    "tfeatures = {\n",
    "    'scosine':tscosine,\n",
    "    'kcosine':tkcosine,\n",
    "}\n",
    "columns=list(tfeatures.keys())\n",
    "columns.append('class')\n",
    "\n",
    "df2 = DataFrame(columns=columns)\n",
    "for i in range(len(data)):\n",
    "    row = []\n",
    "    for feature in tfeatures:\n",
    "        row.append(features[feature](data.sentence1[i],data.sentence2[i]))\n",
    "\n",
    "    row.append(data.iloc[i]['class'])\n",
    "    df2.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5801, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7168294083859851"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = df2.as_matrix(list(features.keys()))\n",
    "y2 = df2.as_matrix(['class']).ravel()\n",
    "Xs2 = scale(x2,with_mean=True,with_std=True,axis=0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(Xs2, y2, test_size=0.3, random_state=4)\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train2, y_train2)\n",
    "y_pred2 = clf.predict(X_test2)\n",
    "metrics.accuracy_score(y_test2, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook the model Tf-Idf was applied to a real problem named *Paraphrase Recognition*. The results can be interpreted as bad, but the majority of the measures that use this model created in the other notebooks has not been included. It is recommended that more similarity measures and different word embedding models must be tested to have a sustainable and scientific answer about the importance or influence of the word embedding methods to similarity problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
