{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity using Tf-Idf Model\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of TfIdf Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create TfIdf models with Gensim Tf-Idf implementations, using NLTK preprocessing. Then introduce how to extract features from this text representation, and finally how to measure text similarity using previous results.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- TfIdf model generation\n",
    "- Wrangling data from BOW to numpy objects\n",
    "- Text similarity measures examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TfIdf?\n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. [(Salton1983)](#Salton1983)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [[word for word in nltk.word_tokenize(doc)] for doc in doc_collection]\n",
    "\n",
    "print(len(tokenized_text))\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#remove stop words\n",
    "texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the TfIdf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 2.258864164352417\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "# Create dictionary with tid to token mappings (or alternatively load one)\n",
    "id2word = Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "id2word.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "bow_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "#generating the tf-idf model\n",
    "tfidf = TfidfModel(bow_corpus,id2word=id2word)\n",
    "end = time.time()-init\n",
    "print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5030, 1)]\n",
      "[(5030, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(id2word.doc2bow(['money']))\n",
    "print(tfidf[id2word.doc2bow(['money'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling Data\n",
    "\n",
    "* First: From string-sentences to bow representation of a sentence.\n",
    "* Second: From bow representation to numerical-list representation of a sentence.\n",
    "* Third: From numerical-list vector to numerical-vector (numpy) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sentence1, sentence2, model):\n",
    "    \n",
    "    #from raw sent to bowvec sent\n",
    "    sent1 = sentence1.split()\n",
    "    sent2 = sentence2.split()\n",
    "\n",
    "    bowvec_sent1 = id2word.doc2bow(sent1)\n",
    "    bowvec_sent2 = id2word.doc2bow(sent2)\n",
    "\n",
    "    bowvec_sent1_tfidf = tfidf[bowvec_sent1]\n",
    "    bowvec_sent2_tfidf = tfidf[bowvec_sent2]\n",
    "    \n",
    "    #from bowvec to numerical list sent\n",
    "    \n",
    "    nvec1 = []\n",
    "    nvec2 = []\n",
    "    vec1 = dict(bowvec_sent1_tfidf)\n",
    "    vec2 = dict(bowvec_sent2_tfidf)\n",
    "    words = set(vec1.keys()).union(vec2.keys())\n",
    "    for word in words:\n",
    "        nvec1.append(vec1.get(word,0.0))\n",
    "        nvec2.append(vec2.get(word,0.0))\n",
    "        \n",
    "    #from numerical list sent to numpy vec\n",
    "    nvec_sent1_tfidf = np.asarray(nvec1)\n",
    "    nvec_sent2_tfidf = np.asarray(nvec2)\n",
    "    A = nvec_sent1_tfidf.reshape(1,-1)\n",
    "    B = nvec_sent2_tfidf.reshape(1,-1)\n",
    "    \n",
    "    return bowvec_sent1_tfidf,bowvec_sent2_tfidf,nvec1,nvec2, A, B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow-Vec with tfidf values of sent 1 [(20645, 0.6507150320816605), (61572, 0.7593220311718628)]\n",
      "Numerical list of sent 1 [0.0, 0.7593220311718628, 0.6507150320816605]\n",
      "Numpy vector of sent 1 [[ 0.          0.75932203  0.65071503]]\n"
     ]
    }
   ],
   "source": [
    "bowvec_sent1_tfidf,bowvec_sent2_tfidf,nvec1,nvec2, A, B = preproc_data(sentence1,sentence2,tfidf)\n",
    "print('Bow-Vec with tfidf values of sent 1', bowvec_sent1_tfidf)\n",
    "print('Numerical list of sent 1',nvec1)\n",
    "print('Numpy vector of sent 1', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn TfIdf-Cosine sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2428008742309789"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(A,B)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2428008742309789"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1s = 'girl run hall'\n",
    "sentence2s = 'Alice run hall'\n",
    "bowvec_sent1s_tfidf,bowvec_sent2s_tfidf,nvec1s,nvec2s, As, Bs = preproc_data(sentence1s,sentence2s,tfidf)\n",
    "cosine_similarity(As,Bs)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy TfIdf-Cosine sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757199125769\n",
      "0.757199125769\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "print(cosine_scipy(nvec1,nvec2))\n",
    "print(cosine_scipy(nvec1s,nvec2s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim tfidf.n_similarity\n",
    "\n",
    "Do not exist this kind of method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "Due to the unmanagability of TfIdf Gensim object, and the few examples I could get, I decided to create de TfIdf with sklearn and then manipulated, to get the tfidf vector o a word, see the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63341, 21)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer(min_df=1, lowercase=False) #note: by default CountVectorizer, TfidfVectorizer use lowercase=True\n",
    "corpus = load_files('./',categories=['gutenberg'])\n",
    "TfIdfMatrix = vectorizer.fit_transform(corpus.data)\n",
    "pdTfIdf = pd.DataFrame(TfIdfMatrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "pdTfIdf = pdTfIdf.T\n",
    "pdTfIdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.39911051e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   4.24280334e-04,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.15835370e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   3.26450029e-04]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdTfIdf.loc['Alice'].values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157856019931182"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = ['the','girl','run','into','the','hall']\n",
    "sent2 = ['Here','Alice','run','to','the','hall']\n",
    "\n",
    "def harmonic_best_pair_word_sim(sent1,sent2, pdTfIdf):\n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        winp = pdTfIdf.loc[wi].values.reshape(1,-1)\n",
    "        for wc in sent2:\n",
    "            wcnp = pdTfIdf.loc[wc].values.reshape(1,-1)\n",
    "            m = max(m, cosine_similarity(winp,wcnp))\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        wcnp = pdTfIdf.loc[wc].values.reshape(1,-1)\n",
    "        for wi in sent1:\n",
    "            winp = pdTfIdf.loc[wi].values.reshape(1,-1)\n",
    "            m = max(m, cosine_similarity(winp,wcnp))\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim\n",
    "\n",
    "harmonic_best_pair_word_sim(sent1,sent2, pdTfIdf)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.889675805486\n"
     ]
    }
   ],
   "source": [
    "#With stopword filtering\n",
    "sent1 = ['girl','run','hall']\n",
    "sent2 = ['Alice','run','hall']\n",
    "print(harmonic_best_pair_word_sim(sent1,sent2,pdTfIdf)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim TfIdf-Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610041216037\n",
      "0.919727979992\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "\n",
    "print(hellinger(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print(hellinger(A,B))\n",
    "print(kullback_leibler(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Cosine: 0.2428008742309789\n",
      "Gensim Cosine, filtering stopwords: 0.2428008742309789\n",
      "Gensim Jaccard: 0.880566019484764\n"
     ]
    }
   ],
   "source": [
    "print('Gensim Cosine:',cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print('Gensim Cosine, filtering stopwords:',cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print('Gensim Jaccard:',jaccard(bowvec_sent1_tfidf,bowvec_sent2_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.75932203,  0.65071503]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* As you can test the TfIdf doesn't have a fast or parallel solution.\n",
    "* In Gensim TfIdf model is generated from bowvecs.\n",
    "* There is a good variation between Cosine, Word Overlap and Hellinger, this could be interesting to analize in a big dataset.\n",
    "* Interesting too is that Gensim and Sklearn cosine haven't the same result.\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "# References\n",
    "\n",
    "\n",
    "[1] *[Salton1983]* Salton, G; McGill, M. J. (1986). Introduction to modern information retrieval. McGraw-Hill. \n",
    "ISBN 978-0-07-054484-0.\n",
    "<a id='Salton1983'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
