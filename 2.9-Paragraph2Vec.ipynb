{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity in Paragraph2Vec Text Representation\n",
    "\n",
    "*Gensim software examples.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create paragraph2vec models with Gensim and NLTK. Then introduce how to extract information from both text representation, and finally how to measure word similarity.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- paragraph2vec model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Gensim\n",
    "\n",
    "Gensim is a Python library for *topic modelling*, *document indexing*\n",
    "and *similarity retrieval* with large corpora. Target audience is the\n",
    "*natural language processing* (NLP) and *information retrieval* (IR)\n",
    "community. [Gensim Documentation](Gensim Doc)\n",
    "\n",
    "## About NLTK\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language\n",
    "processing and text analytics. Originally designed for teaching, it has been adopted in the\n",
    "industry for research and development due to its usefulness and breadth of coverage. NLTK\n",
    "is often used for rapid prototyping of text processing programs and can even be used in\n",
    "production applications. [(Perkins2014)](#Perkins2014)\n",
    "\n",
    "## What is Paragrah2Vec?\n",
    "\n",
    "Paragrah2Vec ...[(xxx)](#xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smart_open\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "This first method to load the whole text collection is based on \"os\" module, this is only a code snippet to practice a different ways to do it. NLTK, numpy, and other libraries have it's own methods to do the same process.\n",
    "\n",
    "In this case a new corpus with one document per line is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = ''\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection += doc.read()+'\\n'\n",
    "\n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "with open('data/all_gutenberg', 'w') as f:\n",
    "    f.write(doc_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Paragraph2Vec Model\n",
    "\n",
    "_Note:_ This model apply a lowercarse of all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Model Generated in 134 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    p2v= Doc2Vec.load('models/gutenberg-p2v.model')\n",
    "    print('Doc2Vec Model Generated in 134 seconds')\n",
    "except:\n",
    "    init = time.time()\n",
    "    corpus = list(read_corpus('data/all_gutenberg'))\n",
    "    paraph2vec = Doc2Vec(corpus, vector_size=300, window=8, min_count=5, workers=4)\n",
    "    end = time.time()-init\n",
    "    paraph2vec.save('models/gutenberg_p2v.model')\n",
    "    print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6150048 , -0.13362435, -0.17901617, -0.13512719, -0.18723272,\n",
       "       -0.7113043 ,  0.3514476 , -0.50977117, -0.5001958 , -0.39531764],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv['alice'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Paragraph2Vec-Cosine Sentence Similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to numpy paragraph vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.split()\n",
    "sent2 = sentence2.split()\n",
    "\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "\n",
    "sent1sl = sent1s.split()\n",
    "sent2sl = sent2s.split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00238911, -0.01297163, -0.03777327,  0.0410721 , -0.00120385,\n",
       "       -0.03433726,  0.01768394, -0.04179351, -0.01550062,  0.02970592],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1_p2v = p2v.infer_vector(sent1)\n",
    "vec_sent2_p2v = p2v.infer_vector(sent2)\n",
    "\n",
    "#print the Paragraph2vector of the sentence 1\n",
    "print(len(vec_sent1_p2v))\n",
    "vec_sent1_p2v[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51463765"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(vec_sent1_p2v.reshape(1,-1),vec_sent2_p2v.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling Data\n",
    "\n",
    "From string sentences to word for word paragraph2vec model numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent1, sent2, model):\n",
    "\n",
    "    sentence1 = sent1.split()\n",
    "    sentence2 = sent2.split()\n",
    "    \n",
    "    p2v_sent1 = []\n",
    "    p2v_sent2 = []\n",
    "\n",
    "    for i,word in enumerate(sentence1):\n",
    "        try:\n",
    "            p2v_sent1.append(paraph2vec.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i,word in enumerate(sentence2):\n",
    "        try:\n",
    "            p2v_sent2.append(paraph2vec.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    p2v_sent1 = sum(np.asarray(p2v_sent1))\n",
    "    p2v_sent2 = sum(np.asarray(p2v_sent2))\n",
    "    \n",
    "    A = p2v_sent1.reshape(1,-1)\n",
    "    B = p2v_sent2.reshape(1,-1)\n",
    "    \n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.10360318, -1.6545863 , -0.962557  ,  0.65428835,  0.96878296,\n",
       "       -2.354933  , -0.3036998 ,  0.33115143,  0.29521412,  0.64372826],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v_sent1, p2v_sent2 = preproc_data(sentence1,sentence2,paraph2vec)\n",
    "print(len(p2v_sent1[0]))\n",
    "p2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7995827"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(p2v_sent1,p2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8869797"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "p2v_sent1s, p2v_sent2s = preproc_data(sent1s,sent2s,paraph2vec)\n",
    "cosine_similarity(p2v_sent1s,p2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20041728019714355\n",
      "0.11302042007446289\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(p2v_sent1,p2v_sent2))\n",
    "print(cosine_scipy(p2v_sent1s,p2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim paragraph2vec sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-abe763561321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkullback_leibler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhellinger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcossim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_sent1_p2v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec_sent2_p2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for the next line to work the model must contain all appearing words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#paraph2vec.n_similarity(sentence1,sentence2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wordembd/lib/python3.5/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mcossim\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \"\"\"\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvec1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "cossim(vec_sent1_p2v,vec_sent2_p2v)\n",
    "\n",
    "#for the next line to work the model must contain all appearing words\n",
    "#paraph2vec.n_similarity(sentence1,sentence2)\n",
    "\n",
    "#paraph2vec.wv.n_similarity(sentence1,sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraph2vec.n_similarity(['the','girl','run','into','the','hall'],['here','alice','run','to','the','hall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraph2vec.n_similarity(['girl','run','hall'],['alice','run','hall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraph2vec.n_similarity(['the','boy','eat','red','apple'],\n",
    "                   ['here','alice','run','to','the','hall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v infer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing initial infer_vector similarity\n",
    "vec_sent1_p2v = vec_sent1_p2v.reshape(1,-1)\n",
    "vec_sent2_p2v = vec_sent2_p2v.reshape(1,-1)\n",
    "cosine_similarity(vec_sent1_p2v,vec_sent2_p2v)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer_vector similarity filtering stopwords\n",
    "vec_sent1s_p2v = paraph2vec.infer_vector(sent1s.split()).reshape(1,-1)\n",
    "vec_sent2s_p2v = paraph2vec.infer_vector(sent2s.split()).reshape(1,-1)\n",
    "cosine_similarity(vec_sent1s_p2v,vec_sent2s_p2v)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v.similarity\n",
    "\n",
    "**Warning:** all the words must be converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "vec_sent1 = paraph2vec.wv[sent1]\n",
    "vec_sent2 = paraph2vec.wv[sent2]\n",
    "\n",
    "#cosine(vec_sent1,vec_sent2)\n",
    "vec_sent1_ = sum(vec_sent1).reshape(1,-1)\n",
    "vec_sent2_ = sum(vec_sent2).reshape(1,-1)\n",
    "\n",
    "cosine_similarity(vec_sent1_,vec_sent2_)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_cosine_sim(sent1, sent2,p2v):\n",
    "    for i,word in enumerate(sent1):\n",
    "        if i == 0:\n",
    "            sent1_p2v = p2v.wv[word]\n",
    "        else:\n",
    "            sent1_p2v+= p2v.wv[word]\n",
    "\n",
    "    for i,word in enumerate(sent2):\n",
    "        if i == 0:\n",
    "            sent2_p2v = p2v.wv[word]\n",
    "        else:\n",
    "            sent2_p2v+= p2v.wv[word]\n",
    "\n",
    "    # get the sentence vector similarity\n",
    "    return 1-cosine_scipy(sent1_p2v,sent2_p2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector_cosine_sim(sent1,sent2,paraph2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like if paragraph2vec had a sparcity problem, due to that word vectors are to slow.\n",
    "Also the model if you test the *wv* method to many times the numers approximate to 0.999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap Similarity\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraph2vec.similarity('girl','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(string1,string2):\n",
    "    p=0\n",
    "    for wi in string1:\n",
    "        m = 0\n",
    "        for wc in string2:\n",
    "            try:\n",
    "                m = max(m, paraph2vec.similarity(wi,wc))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(string1)\n",
    "\n",
    "    q=0\n",
    "    for wc in string2:\n",
    "        m = 0\n",
    "        for wi in string1:\n",
    "            try:\n",
    "                m = max(m, paraph2vec.similarity(wi,wc))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(string2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentence w2v_harmonic_best_pair_word with stopwords', harmonic_best_pair_word_sim(sent1,sent2,w2v))\n",
    "print('Sentence w2v_harmonic_best_pair_word without stopwords',harmonic_best_pair_word_sim(sent1,sent2,w2v))\n",
    "print('Different sentence w2v_harmonic_best_pair_word similarity', harmonic_best_pair_word_sim(sent3,sent2,w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textsim Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/abelm/')\n",
    "import textsim\n",
    "from textsim.tokendists import jaccard_distance\n",
    "print('Textsim Jaccard', jaccard_distance(sent1,sent2))\n",
    "print('Textsim Jaccard, stopwords_filter=yes', jaccard_distance('girl run hall','Alice eat hall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Same as Word2Vec this model doesn't works with bow structure, it represent a word as a vector of *size parameter* value length. At the same time this model can infered a vector for a sentence. The experiments shows that with the same corpus and the same sentences the paragraph2vec tends to fail with some words, e.g. 'Alice' or 'Here', this behavior is different to Word2Vec model.\n",
    "\n",
    "* Gensim Hellinger, Cosine, Jaccard, Kullback-Leibler and the others based on bowvec doesn't work.\n",
    "* 0.777 input = str, Jaccard, Textsim, stopwords_filter=no\n",
    "* 0.800 input = str, Jaccard, Textsim, stopwords_filter=yes\n",
    "* 0.433 input = str, Cosine, Textsim-sklearn, stopwords_filter=no\n",
    "* 0.333 input = str, Cosine, Textsim-sklearn, stopwords_filter=yes\n",
    "* 0.773 input = self vec, Cosine, Sklearn\n",
    "* 0.361 input = Doc2Vec infer_vec, Cosine, Sklearn\n",
    "* 0.635 input = str list, Harmonic mean, Best word sim of words in both sentences, stopwords_filter=no\n",
    "* 0.543 input = str list, Harmonic mean, Best word sim of words in both sentences, stopwords_filter=yes\n",
    "\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='referencias'></a>\n",
    "# Referencias\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "[1] *[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "<a id='John2016'></a>\n",
    "[3] *[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. **NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
